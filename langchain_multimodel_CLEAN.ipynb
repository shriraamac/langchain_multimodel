{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812a4dbc-fe04-4b84-bdf9-390045e30806",
   "metadata": {
    "id": "812a4dbc-fe04-4b84-bdf9-390045e30806"
   },
   "source": [
    "# Multi-modal RAG with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": "# Secure environment variable loading\ntry:\n    from dotenv import load_dotenv\n    load_dotenv()  # loads variables from a local .env file (kept out of git)\nexcept Exception as e:\n    print(\"Tip: install python-dotenv to load .env files automatically: pip install python-dotenv\")\n\n# If you previously used custom env var names, you can map them here:\nimport os\nif os.getenv(\"OPENAI_API_KEY\") is None and os.getenv(\"LANGCHAIN_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n\n# Example: initialize your LLM after env vars are loaded\n# from langchain_openai import ChatOpenAI\n# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "ecXPgawqG7XH",
   "metadata": {
    "id": "ecXPgawqG7XH"
   },
   "source": [
    "## SetUp\n",
    "\n",
    "Install the dependencies you need to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133b74f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "133b74f6",
    "outputId": "5f78050b-c98b-4d01-8b3f-c65937c28b93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# for linux\n",
    "!apt-get install poppler-utils tesseract-ocr libmagic-dev\n",
    "\n",
    "# for mac\n",
    "# !brew install poppler tesseract libmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2nDWBTrhn-_M",
   "metadata": {
    "id": "2nDWBTrhn-_M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\A.C.shriraam\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\A.C.shriraam\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\A.C.shriraam\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\A.C.shriraam\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq \"unstructured[all-docs]\" pillow lxml pillow\n",
    "%pip install -Uq chromadb tiktoken\n",
    "%pip install -Uq langchain langchain-community langchain-openai langchain-groq\n",
    "%pip install -Uq python_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91106e31",
   "metadata": {
    "id": "91106e31"
   },
   "outputs": [],
   "source": "import os\n\n# keys for the services we will use\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
  },
  {
   "cell_type": "markdown",
   "id": "74b56bde-1ba0-4525-a11d-cab02c5659e4",
   "metadata": {
    "id": "74b56bde-1ba0-4525-a11d-cab02c5659e4"
   },
   "source": [
    "## Extract the data\n",
    "\n",
    "Extract the elements of the PDF that we will be able to use in the retrieval process. These elements can be: Text, Images, Tables, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ec070",
   "metadata": {
    "id": "e62ec070"
   },
   "source": [
    "### Partition PDF tables, text, and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a046528-8d22-4f4e-a520-962026562939",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "0a046528-8d22-4f4e-a520-962026562939",
    "outputId": "f7cf84f8-f8b8-46b1-ad38-48c62cb81651",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import os\n",
    "\n",
    "# Upload file(s) from your computer\n",
    "uploaded = files.upload()   # You can pick multiple PDFs\n",
    "\n",
    "# Process each uploaded PDF\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "    file_path = filename  # since it's saved in current working dir\n",
    "\n",
    "    # Partition the PDF\n",
    "    chunks = partition_pdf(\n",
    "        filename=file_path,\n",
    "        strategy=\"fast\",               # use \"hi_res\" if you want OCR/table detection\n",
    "        infer_table_structure=True,\n",
    "        extract_image_block_types=[\"Image\", \"Table\"],\n",
    "        extract_image_block_to_payload=False,  # set True if you want base64 payloads\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=8000,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        new_after_n_chars=6000,\n",
    "    )\n",
    "\n",
    "    # Show first 3 extracted elements\n",
    "    for c in chunks[:3]:\n",
    "        print(type(c), c.to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f6733",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "038f6733",
    "outputId": "1155f087-f5fe-4b75-f266-f62e0ab6c4ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get 2 types of elements from the partition_pdf function\n",
    "set([str(type(el)) for el in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccca0db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cccca0db",
    "outputId": "3eab2d97-9bcf-40cc-91cb-5c88c0f1bc36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Text at 0x7e8325310fb0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325373260>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325220dd0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325373e30>,\n",
       " <unstructured.documents.elements.Text at 0x7e832535af90>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325235820>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325377440>,\n",
       " <unstructured.documents.elements.Text at 0x7e832523d970>,\n",
       " <unstructured.documents.elements.Text at 0x7e832523cdd0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832523e0f0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832523f6b0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832523ff80>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325372c60>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325373140>,\n",
       " <unstructured.documents.elements.Text at 0x7e83253706b0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832522aa50>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325372000>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252284d0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832522a510>,\n",
       " <unstructured.documents.elements.Text at 0x7e832535a5a0>,\n",
       " <unstructured.documents.elements.Text at 0x7e83253591f0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325e03ec0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325249c40>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325249670>,\n",
       " <unstructured.documents.elements.Text at 0x7e832524a0c0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832535acf0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832524a0f0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325248950>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325249280>,\n",
       " <unstructured.documents.elements.Text at 0x7e832524b500>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325248230>,\n",
       " <unstructured.documents.elements.Text at 0x7e832524b6e0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325249a60>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325248500>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252496d0>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252495e0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325248410>,\n",
       " <unstructured.documents.elements.Text at 0x7e832524b2c0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832522b7a0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832522ad50>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325235070>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252355b0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325235880>,\n",
       " <unstructured.documents.elements.Text at 0x7e832523c080>,\n",
       " <unstructured.documents.elements.Text at 0x7e832523cb30>,\n",
       " <unstructured.documents.elements.Text at 0x7e832523e9f0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832523cc80>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325255ac0>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252540b0>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252540e0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325254a40>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325254350>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252551c0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325255340>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325237e60>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325370740>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325373fb0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325203d10>,\n",
       " <unstructured.documents.elements.Text at 0x7e83253773b0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832525e360>,\n",
       " <unstructured.documents.elements.Text at 0x7e832525f1d0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832525d070>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325234500>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325234c20>,\n",
       " <unstructured.documents.elements.Text at 0x7e832531cec0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325237b00>,\n",
       " <unstructured.documents.elements.Text at 0x7e832531e5a0>,\n",
       " <unstructured.documents.elements.Text at 0x7e832531e450>,\n",
       " <unstructured.documents.elements.Text at 0x7e832520d430>,\n",
       " <unstructured.documents.elements.Text at 0x7e832520ca10>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325234440>,\n",
       " <unstructured.documents.elements.Text at 0x7e832520df70>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325374380>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252366f0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8327021c70>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325310140>,\n",
       " <unstructured.documents.elements.Text at 0x7e832522ba70>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325248110>,\n",
       " <unstructured.documents.elements.Text at 0x7e832524bb60>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325254d40>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252568a0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325257710>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325256780>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325255b50>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325256270>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325256f00>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252575f0>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325257f50>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325257020>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325374f80>,\n",
       " <unstructured.documents.elements.Text at 0x7e8325375100>,\n",
       " <unstructured.documents.elements.Text at 0x7e83252680b0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each CompositeElement containes a bunch of related elements.\n",
    "# This makes it easy to use these elements together in a RAG pipeline.\n",
    "\n",
    "chunks[2].metadata.orig_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26caebda",
   "metadata": {
    "id": "26caebda"
   },
   "source": [
    "### Separate extracted elements into tables, text, and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326a750",
   "metadata": {
    "id": "8326a750"
   },
   "outputs": [],
   "source": [
    "# separate tables from texts\n",
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    if \"Table\" in str(type(chunk)):\n",
    "        tables.append(chunk)\n",
    "\n",
    "    if \"CompositeElement\" in str(type((chunk))):\n",
    "        texts.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df548e46",
   "metadata": {
    "id": "df548e46"
   },
   "outputs": [],
   "source": [
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    images_b64.append(el.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "images = get_images_base64(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9582f462",
   "metadata": {
    "id": "9582f462"
   },
   "source": [
    "#### Check what the images look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83158c36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83158c36",
    "outputId": "cb627326-a879-4233-ef97-be4624ed4367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No images found in this document.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    # Decode the base64 string to binary\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    # Display the image\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "# Guard against empty list\n",
    "if images and len(images) > 0:\n",
    "    display_base64_image(images[0])\n",
    "else:\n",
    "    print(\"No images found in this document.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7f52f-bf5c-4ba4-af72-b2ccba59a4cf",
   "metadata": {
    "id": "0aa7f52f-bf5c-4ba4-af72-b2ccba59a4cf"
   },
   "source": [
    "## Summarize the data\n",
    "\n",
    "Create a summary of each element extracted from the PDF. This summary will be vectorized and used in the retrieval process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55862c",
   "metadata": {
    "id": "8b55862c"
   },
   "source": [
    "### Text and Table summaries\n",
    "\n",
    "We don't need a multimodal model to generate the summaries of the tables and the text. I will use open source models available on Groq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3d2bc",
   "metadata": {
    "id": "08b3d2bc"
   },
   "outputs": [],
   "source": [
    "!pip install -Uq langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e6ed2-2132-4748-bdb7-db765f20648d",
   "metadata": {
    "id": "523e6ed2-2132-4748-bdb7-db765f20648d"
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c22e3f-42fb-4a4a-a87a-89f10ba8ab99",
   "metadata": {
    "id": "22c22e3f-42fb-4a4a-a87a-89f10ba8ab99"
   },
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additionnal comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176b374-aef0-48f4-a104-fb26b1dd6922",
   "metadata": {
    "id": "f176b374-aef0-48f4-a104-fb26b1dd6922"
   },
   "outputs": [],
   "source": [
    "import time, math, random, re\n",
    "from typing import List, Any\n",
    "from groq import RateLimitError\n",
    "\n",
    "def _parse_retry_after_seconds(err: Exception) -> float:\n",
    "    \"\"\"Try to parse 'Please try again in Xs' from Groq error message.\"\"\"\n",
    "    m = re.search(r\"try again in ([0-9]+(?:\\.[0-9]+)?)s\", str(err), re.IGNORECASE)\n",
    "    if m:\n",
    "        return float(m.group(1))\n",
    "    return 0.0\n",
    "\n",
    "def summarize_with_rate_limit(\n",
    "    chain,\n",
    "    inputs: List[str],\n",
    "    batch_size: int = 5,\n",
    "    max_concurrency: int = 1,\n",
    "    max_retries: int = 5,\n",
    "    base_sleep: float = 2.0,\n",
    ") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Robust wrapper around chain.batch() to avoid 429s and preserve order.\n",
    "    Returns a list aligned with `inputs`. Failed items contain an error dict.\n",
    "    \"\"\"\n",
    "    n = len(inputs)\n",
    "    results: List[Any] = [None] * n  # preserve original order\n",
    "    # Process in small batches\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        sub = inputs[start:end]\n",
    "\n",
    "        # Retry loop for this mini-batch\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                out = chain.batch(sub, {\"max_concurrency\": max_concurrency})\n",
    "                # Assign back in order\n",
    "                for i, val in enumerate(out):\n",
    "                    results[start + i] = val\n",
    "                break  # done with this batch\n",
    "            except RateLimitError as e:\n",
    "                # Prefer provider's hint; otherwise exp backoff + jitter\n",
    "                hint = _parse_retry_after_seconds(e)\n",
    "                wait = hint if hint > 0 else base_sleep * (2 ** (attempt - 1))\n",
    "                wait += random.uniform(0, 0.5)  # jitter\n",
    "                print(f\"[rate_limit] batch {start}:{end} attempt {attempt}/{max_retries} -> sleeping {wait:.2f}s\")\n",
    "                time.sleep(wait)\n",
    "            except Exception as e:\n",
    "                # Non-rate-limit error: on last attempt, mark errors so we don't lose alignment\n",
    "                if attempt == max_retries:\n",
    "                    print(f\"[error] batch {start}:{end} failed permanently: {e}\")\n",
    "                    for i in range(len(sub)):\n",
    "                        if results[start + i] is None:\n",
    "                            results[start + i] = {\"ok\": False, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "                else:\n",
    "                    wait = base_sleep * (2 ** (attempt - 1)) + random.uniform(0, 0.5)\n",
    "                    print(f\"[warn] batch {start}:{end} attempt {attempt}/{max_retries} error: {e}. Sleeping {wait:.2f}s\")\n",
    "                    time.sleep(wait)\n",
    "\n",
    "        # Optional small pause between mini-batches to smooth usage\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ---- Use it ----\n",
    "# Assumes you already have `summarize_chain` (LangChain Runnable) and `texts` (List[str])\n",
    "text_summaries = summarize_with_rate_limit(\n",
    "    summarize_chain,\n",
    "    texts,\n",
    "    batch_size=5,          # tune smaller if you still see 429\n",
    "    max_concurrency=1,     # keep 1 for Groq \"on_demand\" tiers\n",
    "    max_retries=6,         # a bit more cushion\n",
    "    base_sleep=2.0,        # base for exponential backoff\n",
    ")\n",
    "\n",
    "# Quick sanity check\n",
    "ok = sum(1 for r in text_summaries if not isinstance(r, dict) or r.get(\"ok\", True))\n",
    "print(f\"Completed: {ok}/{len(texts)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d172ad2",
   "metadata": {
    "id": "1d172ad2"
   },
   "outputs": [],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b030d4-2ac5-41b6-9245-fc3ba5771d87",
   "metadata": {
    "id": "67b030d4-2ac5-41b6-9245-fc3ba5771d87"
   },
   "source": [
    "## Load data and summaries to vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d2379",
   "metadata": {
    "id": "bb4d2379"
   },
   "source": [
    "### Create the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d7a34-69e0-49a2-b9f7-1a4e7b26d78f",
   "metadata": {
    "id": "9d8d7a34-69e0-49a2-b9f7-1a4e7b26d78f"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"multi_modal_rag\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf26669",
   "metadata": {
    "id": "2bf26669"
   },
   "source": [
    "### Load the summaries and link the to the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45fb81-46b1-426e-aa2c-01aed4eac700",
   "metadata": {
    "id": "4b45fb81-46b1-426e-aa2c-01aed4eac700"
   },
   "source": [
    "### Check retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd3714",
   "metadata": {
    "id": "90cd3714"
   },
   "source": [
    "## References\n",
    "\n",
    "- [LangChain Inspiration](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb?ref=blog.langchain.dev)\n",
    "- [Multivector Storage](https://python.langchain.com/docs/how_to/multi_vector/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f8e0e",
   "metadata": {
    "id": "2f7f8e0e"
   },
   "source": [
    "# Task\n",
    "Remove all parts of the code that use the OpenAI API key and keep only the parts that use Groq and Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b9082",
   "metadata": {
    "id": "817b9082"
   },
   "source": [
    "## Remove openai api key\n",
    "\n",
    "### Subtask:\n",
    "Remove the OpenAI API key from the environment variables in the setup cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bdf885",
   "metadata": {
    "id": "48bdf885"
   },
   "source": [
    "**Reasoning**:\n",
    "The subtask is to remove the OpenAI API key from the environment variables. The code cell with id `91106e31` sets the environment variables, so I will edit that cell to remove the `OPENAI_API_KEY` line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a606b3",
   "metadata": {
    "id": "47a606b3"
   },
   "outputs": [],
   "source": "import os\n\n# keys for the services we will use\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
  },
  {
   "cell_type": "markdown",
   "id": "1f30be3d",
   "metadata": {
    "id": "1f30be3d"
   },
   "source": [
    "## Remove image summarization with openai\n",
    "\n",
    "### Subtask:\n",
    "Remove the code cells that install `langchain-openai` and generate image summaries using the OpenAI model, as this will no longer be supported.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4400f2",
   "metadata": {
    "id": "0c4400f2"
   },
   "source": [
    "**Reasoning**:\n",
    "The subtask is to remove code related to OpenAI. I will identify and remove the code cells that install langchain-openai, generate image summaries, and print image summaries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ed42d",
   "metadata": {
    "id": "875ed42d"
   },
   "source": [
    "## Update embedding function\n",
    "\n",
    "### Subtask:\n",
    "Replace the `OpenAIEmbeddings` with an embedding function that can be used without an OpenAI API key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f6fde",
   "metadata": {
    "id": "0a8f6fde"
   },
   "source": [
    "**Reasoning**:\n",
    "Install the sentence-transformers library to use HuggingFace embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5bf4f",
   "metadata": {
    "id": "cbe5bf4f"
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41caadc",
   "metadata": {
    "id": "e41caadc"
   },
   "source": [
    "**Reasoning**:\n",
    "Import the necessary class for HuggingFace embeddings and instantiate it to replace OpenAIEmbeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f5d65",
   "metadata": {
    "id": "308f5d65"
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "import uuid\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"multi_modal_rag\", embedding_function=hf_embeddings)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c59ba9",
   "metadata": {
    "id": "55c59ba9"
   },
   "source": [
    "## Load data and summaries to vectorstore\n",
    "\n",
    "### Subtask:\n",
    "Load the text and table summaries (since image summaries will be removed) into the updated vectorstore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5c1292",
   "metadata": {
    "id": "fb5c1292"
   },
   "source": [
    "**Reasoning**:\n",
    "Load the text and table summaries into the updated vectorstore by generating IDs, creating Document objects, adding them to the vectorstore, and storing the original chunks in the docstore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9d4ef",
   "metadata": {
    "id": "26d9d4ef"
   },
   "source": [
    "**Reasoning**:\n",
    "The error indicates that the table_summaries list is empty, which means no tables were extracted and summarized. I need to check if the tables list is empty before attempting to add table summaries to the vectorstore to avoid the error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JImKsEXiD7vz",
   "metadata": {
    "id": "JImKsEXiD7vz"
   },
   "outputs": [],
   "source": [
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables only if there are tables\n",
    "if tables:\n",
    "    table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "    summary_tables = [\n",
    "        Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_tables)\n",
    "    retriever.docstore.mset(list(zip(table_ids, tables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf66e60",
   "metadata": {
    "id": "cbf66e60"
   },
   "source": [
    "## Update rag pipeline\n",
    "\n",
    "### Subtask:\n",
    "Modify the RAG pipeline to use the Groq model for generating the final response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a20bf7",
   "metadata": {
    "id": "08a20bf7"
   },
   "source": [
    "**Reasoning**:\n",
    "The subtask is to modify the RAG pipeline to use the Groq model and remove image handling from the prompt building function. This can be done by updating the `build_prompt` function to only include text context and the user's question, and then replacing `ChatOpenAI` with `ChatGroq` in the chain definitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe23a2",
   "metadata": {
    "id": "65fe23a2"
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from base64 import b64decode\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def parse_docs(docs):\n",
    "    \"\"\"Split base64-encoded images and texts\"\"\"\n",
    "    # Since image summarization is removed, we only need to return text documents\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        text.append(doc)\n",
    "    return {\"texts\": text}\n",
    "\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            context_text += text_element.text # Use .text for CompositeElement\n",
    "\n",
    "    # construct prompt with context (only text)\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question based only on the following context, which can include text and tables.\n",
    "    Context: {context_text}\n",
    "    Question: {user_question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(content=prompt_content),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "groq_model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | groq_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt)\n",
    "        | groq_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496c32e5",
   "metadata": {
    "id": "496c32e5"
   },
   "source": [
    "## Test retrieval\n",
    "\n",
    "### Subtask:\n",
    "Test the retrieval process with the updated setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf944d0",
   "metadata": {
    "id": "8cf944d0"
   },
   "source": [
    "**Reasoning**:\n",
    "Test the updated RAG pipeline by invoking the `chain` with a test question and printing the response, then invoke `chain_with_sources` with the same question and print its response, including the text context and page numbers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e731e",
   "metadata": {
    "id": "406e731e"
   },
   "outputs": [],
   "source": [
    "test_question = \"Where is the train going?\"\n",
    "\n",
    "# Invoke the chain and print the response\n",
    "response = chain.invoke(test_question)\n",
    "print(\"Response from chain:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Invoke the chain_with_sources and print the response and sources\n",
    "response_with_sources = chain_with_sources.invoke(test_question)\n",
    "print(\"Response from chain_with_sources:\")\n",
    "print(\"Response:\", response_with_sources['response'])\n",
    "\n",
    "print(\"\\nContext:\")\n",
    "for text in response_with_sources['context']['texts']:\n",
    "    print(text.text) # Use .text for CompositeElement\n",
    "    # Check if page_number is in metadata before accessing\n",
    "    if hasattr(text.metadata, 'page_number'):\n",
    "        print(\"Page number: \", text.metadata.page_number)\n",
    "    else:\n",
    "        print(\"Page number: Not available\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dfb7e4",
   "metadata": {
    "id": "58dfb7e4"
   },
   "source": [
    "**Reasoning**:\n",
    "The error indicates that `CompositeElement` objects do not have a `page_content` attribute. I need to access the text content of these objects using their `text` attribute instead. I will correct the code in the `build_prompt` function to use `text_element.text` and re-run the tests for both `chain` and `chain_with_sources`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sin-fhCREISb",
   "metadata": {
    "id": "sin-fhCREISb"
   },
   "outputs": [],
   "source": [
    "def parse_docs(docs):\n",
    "    \"\"\"Split base64-encoded images and texts\"\"\"\n",
    "    # Since image summarization is removed, we only need to return text documents\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        text.append(doc)\n",
    "    return {\"texts\": text}\n",
    "\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            context_text += text_element.text # Use .text for CompositeElement\n",
    "\n",
    "    # construct prompt with context (only text)\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question based only on the following context, which can include text and tables.\n",
    "    Context: {context_text}\n",
    "    Question: {user_question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(content=prompt_content),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "groq_model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | groq_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt)\n",
    "        | groq_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")\n",
    "\n",
    "test_question = \"What is the attention mechanism?\"\n",
    "\n",
    "# Invoke the chain and print the response\n",
    "response = chain.invoke(test_question)\n",
    "print(\"Response from chain:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Invoke the chain_with_sources and print the response and sources\n",
    "response_with_sources = chain_with_sources.invoke(test_question)\n",
    "print(\"Response from chain_with_sources:\")\n",
    "print(\"Response:\", response_with_sources['response'])\n",
    "\n",
    "print(\"\\nContext:\")\n",
    "for text in response_with_sources['context']['texts']:\n",
    "    print(text.text) # Use .text for CompositeElement\n",
    "    # Check if page_number is in metadata before accessing\n",
    "    if hasattr(text.metadata, 'page_number'):\n",
    "        print(\"Page number: \", text.metadata.page_number)\n",
    "    else:\n",
    "        print(\"Page number: Not available\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7940ad",
   "metadata": {
    "id": "5b7940ad"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "\n",
    "* The OpenAI API key was successfully removed from the environment variable setup.\n",
    "* Attempts to directly delete code cells using `%%delete_cell` magic commands failed due to lack of support in the environment.\n",
    "* `OpenAIEmbeddings` was successfully replaced with `HuggingFaceEmbeddings` using the 'all-MiniLM-L6-v2' model for document embedding.\n",
    "* An initial error occurred when attempting to add an empty list of table summaries to the vectorstore, which was resolved by adding a check for non-empty tables.\n",
    "* The RAG pipeline was updated to use `ChatGroq` with the \"llama-3.1-8b-instant\" model for generating responses.\n",
    "* The `build_prompt` function was modified to only include text context, removing image handling logic.\n",
    "* During the testing phase, an error was encountered when trying to access the text content of retrieved documents using `.page_content`; this was corrected by using the `.text` attribute for `CompositeElement` objects.\n",
    "* The final test confirmed that the updated RAG pipeline, using Groq and Langchain, successfully retrieves relevant text documents and generates responses.\n",
    "\n",
    "### Insights or Next Steps\n",
    "\n",
    "* Consider exploring alternative methods for removing code cells if direct deletion magic commands are not supported in the environment.\n",
    "* Address the `LangChainDeprecationWarning` for `HuggingFaceEmbeddings` by updating to the recommended `langchain-huggingface` package in future iterations.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2bf26669",
    "4b45fb81-46b1-426e-aa2c-01aed4eac700"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
